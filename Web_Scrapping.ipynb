{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importamos las librerías necesarias, vamos a usar requests para hacer la petición a la página web y BeautifulSoup para parsear el HTML. Luego pandas para guardar los datos en un DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "rw1s_TCMcuOZ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos funciones utiles para el scraping.\n",
        "* **Soup_page:** Rcibe como parametro una url y devuelte el objeto soup de la misma.\n",
        "\n",
        "\n",
        "* **ExtraerInfoLibros:** Recibe como parametro el objeto Soup de la pagina especifica de un libro, extrae la informacion del mismo y devuelve un diccionario con los datos extraidos para luego ser insertados en un DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "bpEGH8EhdKNn"
      },
      "outputs": [],
      "source": [
        "# Función para obtener el objeto Soup de una URL\n",
        "def Soup_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "# Funcion que extrae la informacion de cada libro \n",
        "def ExtraerInfoLibros(book_soup, url):\n",
        "    table = book_soup.find('table', class_='bibrec')\n",
        "    rows = table.find_all('tr')\n",
        "\n",
        "    # Inicializa las variables para almacenar los datos\n",
        "    author = title = language = release_date = summary = None\n",
        "    subjects = []\n",
        "\n",
        "    for row in rows:\n",
        "        th = row.find('th')\n",
        "        td = row.find('td')\n",
        "\n",
        "        if th and td:\n",
        "            if 'Author' in th.text:\n",
        "                author = td.text.strip()\n",
        "            elif 'Title' in th.text:\n",
        "                title = td.text.strip()\n",
        "            elif 'Language' in th.text:\n",
        "                language = td.text.strip()\n",
        "            elif 'Release Date' in th.text:\n",
        "                release_date = td.text.strip()\n",
        "            elif 'Summary' in th.text:\n",
        "                summary = td.text.strip().replace(\"(This is an automatically generated summary.)\", \"\").strip()\n",
        "                # Eliminamos comillas dobles al principio y al final del texto\n",
        "                summary = summary.strip('\"')\n",
        "            elif 'Subject' in th.text:\n",
        "                subject_links = td.find_all('a')\n",
        "                for link in subject_links:\n",
        "                    subjects.append(link.text.strip())\n",
        "\n",
        "    # Convertimos la lista de subjects en una cadena separada por comas\n",
        "    subjects_string = \",\".join(subjects)\n",
        "\n",
        "    # Creamos un diccionario con los datos extraídos\n",
        "    return {\n",
        "        'Title': title,\n",
        "        'Author': author,\n",
        "        'Summary': summary,\n",
        "        'Subjects': subjects_string,\n",
        "        'Release Date': release_date,\n",
        "        'Language': language,\n",
        "        'Url': url,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urJA8LAlfBf0",
        "outputId": "cc6afa8a-a74a-42e7-a079-a412f88d0b64"
      },
      "outputs": [],
      "source": [
        "# Obtenemos el objeto Soup de la página donde se encuentran los 1000 libros\n",
        "url = \"https://www.gutenberg.org/browse/scores/top1000.php#books-last1\"\n",
        "soup = Soup_page(url)\n",
        "\n",
        "# Buscamos la etiqueta <div> con la clase 'page_content' que dentro suyo se encuentra la lista con todos los links a la descripcion de cada libro\n",
        "book_elements = soup.find_all('div', class_='page_content')\n",
        "urls = []\n",
        "base_url = 'https://www.gutenberg.org'\n",
        "\n",
        "counter = 0\n",
        "for div in book_elements:\n",
        "    # Buscamos la etiqueta el <ol> dentro del <div>\n",
        "    ol = div.find('ol')\n",
        "    if ol:\n",
        "        # Recorremos todass las etiquetas <li> dentro del <ol>\n",
        "        for li in ol.find_all('li'):\n",
        "            # Buscamos la etiqueta <a> dentro de cada etiqueta <li>\n",
        "            a_tag = li.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                # Obtenemos la URL de la etiqueta <a>\n",
        "                relative_url = a_tag['href']\n",
        "                full_url = base_url + relative_url\n",
        "                urls.append(full_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFal1NHfkTIS"
      },
      "source": [
        "Ahora vamos a recorrer cada link, obtener el Soup de cada pagina y guardar la informacion respectiva de cada libro en un dataframe de Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "vscode": {
          "languageId": "ruby"
        }
      },
      "outputs": [],
      "source": [
        "# Inicializamos un DataFrame vacío para luego llenarlo con la informacion de los libros\n",
        "df_libros = pd.DataFrame(columns=['Title', 'Author', 'Summary', 'Subjects', 'Release Date', 'Language', 'Url'])\n",
        "\n",
        "for url in urls:\n",
        "    book_soup = soup_page(url)  # Obtén el soup de la página\n",
        "    book_data = ExtraerInfoLibros(book_soup, url)  # Extrae los datos del libro\n",
        "    df_libros = pd.concat([df_libros, pd.DataFrame([book_data])], ignore_index=True)  # Agrega la fila al DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "vscode": {
          "languageId": "ruby"
        }
      },
      "outputs": [],
      "source": [
        "# Guardamos el DataFrame en un archivo CSV\n",
        "df_libros.to_csv('df_libros.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
