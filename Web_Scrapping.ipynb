{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías necesarias, vamos a usar requests para hacer la petición a la página web y BeautifulSoup para parsear el HTML. Luego pandas para guardar los datos en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rw1s_TCMcuOZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Francisco\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funciones utiles para el scraping.\n",
    "* **Soup_page:** Rcibe como parametro una url y devuelte el objeto soup de la misma.\n",
    "\n",
    "\n",
    "* **ExtraerInfoLibros:** Recibe como parametro el objeto Soup de la pagina especifica de un libro, extrae la informacion del mismo y devuelve un diccionario con los datos extraidos para luego ser insertados en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "bpEGH8EhdKNn"
   },
   "outputs": [],
   "source": [
    "# Función para obtener el objeto Soup de una URL\n",
    "def Soup_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Funcion que extrae la informacion de cada libro \n",
    "def ExtraerInfoLibros(book_soup, url):\n",
    "    table = book_soup.find('table', class_='bibrec')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Inicializa las variables para almacenar los datos\n",
    "    author = title = language = release_date = summary = None\n",
    "    subjects = []\n",
    "\n",
    "    for row in rows:\n",
    "        th = row.find('th')\n",
    "        td = row.find('td')\n",
    "\n",
    "        if th and td:\n",
    "            if 'Author' in th.text:\n",
    "                author = td.text.strip()\n",
    "            elif 'Title' in th.text:\n",
    "                title = td.text.strip()\n",
    "            elif 'Language' in th.text:\n",
    "                language = td.text.strip()\n",
    "            elif 'Release Date' in th.text:\n",
    "                release_date = td.text.strip()\n",
    "            elif 'Summary' in th.text:\n",
    "                summary = td.text.strip().replace(\"(This is an automatically generated summary.)\", \"\").strip()\n",
    "                # Eliminamos comillas dobles al principio y al final del texto\n",
    "                summary = summary.strip('\"')\n",
    "            elif 'Subject' in th.text:\n",
    "                subject_links = td.find_all('a')\n",
    "                for link in subject_links:\n",
    "                    subjects.append(link.text.strip())\n",
    "\n",
    "    # Convertimos la lista de subjects en una cadena separada por comas\n",
    "    subjects_string = \",\".join(subjects)\n",
    "\n",
    "    # Creamos un diccionario con los datos extraídos\n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Author': author,\n",
    "        'Summary': summary,\n",
    "        'Subjects': subjects_string,\n",
    "        'Release Date': release_date,\n",
    "        'Language': language,\n",
    "        'Url': url,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urJA8LAlfBf0",
    "outputId": "cc6afa8a-a74a-42e7-a079-a412f88d0b64"
   },
   "outputs": [],
   "source": [
    "# Obtenemos el objeto Soup de la página donde se encuentran los 1000 libros\n",
    "url = \"https://www.gutenberg.org/browse/scores/top1000.php#books-last1\"\n",
    "soup = Soup_page(url)\n",
    "\n",
    "# Buscamos la etiqueta <div> con la clase 'page_content' que dentro suyo se encuentra la lista con todos los links a la descripcion de cada libro\n",
    "book_elements = soup.find_all('div', class_='page_content')\n",
    "urls = []\n",
    "base_url = 'https://www.gutenberg.org'\n",
    "\n",
    "counter = 0\n",
    "for div in book_elements:\n",
    "    # Buscamos la etiqueta el <ol> dentro del <div>\n",
    "    ol = div.find('ol')\n",
    "    if ol:\n",
    "        # Recorremos todass las etiquetas <li> dentro del <ol>\n",
    "        for li in ol.find_all('li'):\n",
    "            # Buscamos la etiqueta <a> dentro de cada etiqueta <li>\n",
    "            a_tag = li.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                # Obtenemos la URL de la etiqueta <a>\n",
    "                relative_url = a_tag['href']\n",
    "                full_url = base_url + relative_url\n",
    "                urls.append(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFal1NHfkTIS"
   },
   "source": [
    "Ahora vamos a recorrer cada link, obtener el Soup de cada pagina y guardar la informacion respectiva de cada libro en un dataframe de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Inicializamos un DataFrame vacío para luego llenarlo con la informacion de los libros\n",
    "df_libros = pd.DataFrame(columns=['Title', 'Author', 'Summary', 'Subjects', 'Release Date', 'Language', 'Url'])\n",
    "\n",
    "for url in urls:\n",
    "    book_soup = soup_page(url)  # Obtén el soup de la página\n",
    "    book_data = ExtraerInfoLibros(book_soup, url)  # Extrae los datos del libro\n",
    "    df_libros = pd.concat([df_libros, pd.DataFrame([book_data])], ignore_index=True)  # Agrega la fila al DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos el DataFrame en un archivo CSV\n",
    "df_libros.to_csv('df_libros.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Cargar un modelo preentrenado como BERT\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset de juegos de mesa\n",
    "df_juegos_de_mesa = pd.read_csv('bgg_database.csv')\n",
    "# Dataset de películas\n",
    "df_peliculas = pd.read_csv('IMDB-Movie-Data.csv')\n",
    "# Dataset de películas\n",
    "df_libros = pd.read_csv('df_libros.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_peliculas['full_text'] = df_peliculas['Genre'] + ' ' + df_peliculas['Description']\n",
    "df_peliculas['full_text'] = df_peliculas['full_text'].fillna('') # Rellenar los valores nulos con un string vacío\n",
    "\n",
    "df_libros['full_text'] = df_libros['Subjects'] + ' ' + df_libros['Summary']\n",
    "df_libros['full_text'] = df_libros['full_text'].fillna('') # Rellenar los valores nulos con un string vacío\n",
    "\n",
    "df_juegos_de_mesa['full_text'] = df_juegos_de_mesa['categories'] + ' ' + df_juegos_de_mesa['description']\n",
    "df_juegos_de_mesa['full_text'] = df_juegos_de_mesa['full_text'].fillna('') # Rellenar los valores nulos con un string vacío"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peliculas OK!\n",
      "Libros OK!\n",
      "Juegos OK!\n"
     ]
    }
   ],
   "source": [
    "# Para películas\n",
    "peliculas_embeddings = model.encode(df_peliculas['full_text'].tolist())\n",
    "df_peliculas['embeddings'] = [np.array(embed) for embed in peliculas_embeddings]\n",
    "print('Peliculas OK!')\n",
    "\n",
    "# Para libros\n",
    "libros_embeddings = model.encode(df_libros['full_text'].tolist())\n",
    "df_libros['embeddings'] = [np.array(embed) for embed in libros_embeddings]\n",
    "print('Libros OK!')\n",
    "\n",
    "# Para juegos\n",
    "juegos_embeddings = model.encode(df_juegos_de_mesa['full_text'].tolist())\n",
    "df_juegos_de_mesa['embeddings'] = [np.array(embed) for embed in juegos_embeddings]\n",
    "print('Juegos OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_peliculas.to_csv('./datasets_embeddings/df_peliculas_emb.csv', index=False)\n",
    "df_libros.to_csv('./datasets_embeddings/df_libros_emb.csv', index=False)\n",
    "df_juegos_de_mesa.to_csv('./datasets_embeddings/df_juegos_de_mesa_emb.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
